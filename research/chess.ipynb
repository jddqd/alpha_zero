{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(\"CUDA :\", torch.cuda.is_available())\n",
    "# print(\"number of GPU :\", torch.cuda.device_count())\n",
    "# if torch.cuda.is_available():\n",
    "#     print(\"GPU name :\", torch.cuda.get_device_name(0))\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import chess\n",
    "\n",
    "from tqdm.auto import trange\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changement pour chess : action passe de int à tuple (initial_position, final_position)\n",
    "\n",
    "class TicTacToe:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.row_count = 3\n",
    "        self.column_count = 3\n",
    "        self.action_size = self.row_count * self.column_count\n",
    "\n",
    "    # défini dans les champs de la classe chess \n",
    "    def get_initial_state(self):\n",
    "        return np.zeros((self.row_count, self.column_count))\n",
    "    \n",
    "    # Fonction move_piece() dans chess, state = self, initial_position : pièce à bouger, final_position : pièce bougée (forme l'action),\n",
    "    # player est directement dans le champ self.player\n",
    "    def get_next_state(self, state, action, player):\n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        state[row, column] = player\n",
    "        return state\n",
    "\n",
    "\n",
    "    # équivalent : actions()\n",
    "    def get_valid_moves(self, state):\n",
    "        return (state.reshape(-1) == 0).astype(np.uint8)\n",
    "    \n",
    "\n",
    "    # à implémenter dans chess, à l'aide de in_check_possible_moves(), si le retour est vide, alors c'est un échec et mat\n",
    "    # attention à vérifier l'échec avant avec check_status()\n",
    "\n",
    "    # check_status()\n",
    "    # False : action = action()\n",
    "    # True : action = in_check_possible_moves\n",
    "\n",
    "    def check_win(self, state, action):\n",
    "\n",
    "        if action is None:\n",
    "            return False\n",
    "\n",
    "        row = action // self.row_count\n",
    "        column = action % self.column_count\n",
    "        player = state[row, column]\n",
    "\n",
    "        # check row\n",
    "        if np.all(state[row, :] == player):\n",
    "            return True\n",
    "\n",
    "        # check column\n",
    "        if np.all(state[:, column] == player):\n",
    "            return True\n",
    "        \n",
    "        # check diagonal\n",
    "        if row == column and np.all(np.diag(state) == player):\n",
    "            return True\n",
    "\n",
    "        # check anti-diagonal\n",
    "        if row + column == self.row_count - 1 and np.all(np.diag(np.fliplr(state)) == player):\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "    \n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "\n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "\n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player\n",
    "\n",
    "    def get_encoded_state(self, state):\n",
    "        encoded_state = np.stack((state == 1, state == 0, state == -1)).astype(np.float32)\n",
    "\n",
    "        # check for batch dimension and swap axis\n",
    "        if len(state.shape) == 3:\n",
    "            encoded_state = np.swapaxes(encoded_state, 0, 1)\n",
    "\n",
    "\n",
    "        return encoded_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessRL:\n",
    "    def __init__(self):\n",
    "        self.board = chess.Board()\n",
    "        self.row_count = 8\n",
    "        self.column_count = 8\n",
    "        self.all_moves = self._generate_all_moves()\n",
    "        self.action_size = len(self.all_moves)  # Taille du vecteur one-hot\n",
    "        self.move_to_index = {move: i for i, move in enumerate(self.all_moves)}\n",
    "        self.index_to_move = {i: move for i, move in enumerate(self.all_moves)}\n",
    "\n",
    "\n",
    "    def _generate_all_moves(self):\n",
    "        \"\"\"Génère tous les coups UCI possibles aux échecs, indépendamment de l'état du plateau.\"\"\"\n",
    "        all_moves = set()\n",
    "        board = chess.Board.empty()  # Échiquier vide pour générer tous les coups possibles\n",
    "\n",
    "        # Liste de toutes les cases\n",
    "        squares = list(chess.SQUARES)\n",
    "\n",
    "        # Déplacements possibles pour chaque type de pièce\n",
    "        piece_moves = {\n",
    "            chess.PAWN: [chess.Move(from_sq, to_sq) for from_sq in squares for to_sq in squares if abs(from_sq - to_sq) in [8, 16, 7, 9]],\n",
    "            chess.KNIGHT: [chess.Move(from_sq, to_sq) for from_sq in squares for to_sq in squares if abs(from_sq // 8 - to_sq // 8) * abs(from_sq % 8 - to_sq % 8) == 2],\n",
    "            chess.BISHOP: [chess.Move(from_sq, to_sq) for from_sq in squares for to_sq in squares if abs(from_sq // 8 - to_sq // 8) == abs(from_sq % 8 - to_sq % 8)],\n",
    "            chess.ROOK: [chess.Move(from_sq, to_sq) for from_sq in squares for to_sq in squares if (from_sq // 8 == to_sq // 8 or from_sq % 8 == to_sq % 8)],\n",
    "            chess.QUEEN: [chess.Move(from_sq, to_sq) for from_sq in squares for to_sq in squares if (from_sq // 8 == to_sq // 8 or from_sq % 8 == to_sq % 8 or abs(from_sq // 8 - to_sq // 8) == abs(from_sq % 8 - to_sq % 8))],\n",
    "            chess.KING: [chess.Move(from_sq, to_sq) for from_sq in squares for to_sq in squares if max(abs(from_sq // 8 - to_sq // 8), abs(from_sq % 8 - to_sq % 8)) == 1]\n",
    "        }\n",
    "\n",
    "        # Ajoute tous les coups non spécifiques aux règles\n",
    "        for move_list in piece_moves.values():\n",
    "            for move in move_list:\n",
    "                all_moves.add(move.uci())\n",
    "\n",
    "        # Ajoute les promotions de pions (blancs et noirs)\n",
    "        promotion_pieces = ['q', 'r', 'b', 'n']\n",
    "        for file in range(8):\n",
    "            for piece in promotion_pieces:\n",
    "                all_moves.add(f\"{chr(97 + file)}7{chr(97 + file)}8{piece}\")  # Blancs\n",
    "                all_moves.add(f\"{chr(97 + file)}2{chr(97 + file)}1{piece}\")  # Noirs\n",
    "\n",
    "        # Ajoute les roques\n",
    "        all_moves.update([\"e1g1\", \"e1c1\", \"e8g8\", \"e8c8\"])  # Petit et grand roque\n",
    "\n",
    "        # Ajoute les prises en passant (théoriquement possibles)\n",
    "        for file in range(8):\n",
    "            all_moves.add(f\"{chr(97 + file)}5{chr(97 + file + (-1 if file > 0 else 1))}6\")  # Blancs en passant\n",
    "            all_moves.add(f\"{chr(97 + file)}4{chr(97 + file + (-1 if file > 0 else 1))}3\")  # Noirs en passant\n",
    "\n",
    "        return sorted(all_moves)  # Trie pour assurer un ordre fixe\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        return self.board.fen()\n",
    "\n",
    "    def get_next_state(self, state, action, player):\n",
    "        board = chess.Board(state)\n",
    "        move = self.decode_move(action)\n",
    "        if move in board.legal_moves:\n",
    "            board.push(move)\n",
    "        return board.fen()\n",
    "\n",
    "    def get_valid_moves(self, state):\n",
    "        board = chess.Board(state)\n",
    "        valid_moves = np.zeros(self.action_size, dtype=np.uint8)\n",
    "        for i, move in enumerate(board.legal_moves):\n",
    "            idx = np.where(self.encode_move(move) == 1)[0][0]\n",
    "            valid_moves[idx] = 1\n",
    "        return valid_moves\n",
    "\n",
    "    def check_win(self, state, action):\n",
    "        board = chess.Board(state)\n",
    "        if board.is_checkmate():\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        board = chess.Board(state)\n",
    "        if board.is_checkmate():\n",
    "            return 1, True\n",
    "        if board.is_stalemate() or board.is_insufficient_material() or board.is_seventyfive_moves():\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "\n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "\n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "\n",
    "    def change_perspective(self, state, player):\n",
    "        return self.flip_board(state)\n",
    "\n",
    "    def get_encoded_state(self, state):\n",
    "        board = chess.Board(state)\n",
    "        encoded_state = np.zeros((12, 8, 8), dtype=np.float32)\n",
    "        for square in chess.SQUARES:\n",
    "            piece = board.piece_at(square)\n",
    "            if piece:\n",
    "                encoded_state[piece.piece_type - 1, square // 8, square % 8] = 1 if piece.color else -1\n",
    "        return encoded_state\n",
    "\n",
    "    def flip_board(self, state):\n",
    "        board = chess.Board(state)\n",
    "        board.apply_mirror()\n",
    "        return board.fen()\n",
    "\n",
    "    def encode_move(self, move):\n",
    "        \"\"\"Encode un coup UCI en one-hot (index unique).\"\"\"\n",
    "\n",
    "        move_uci = move.uci()\n",
    "        if move_uci not in self.move_to_index:\n",
    "            raise ValueError(f\"Mouvement UCI inconnu : {move_uci}\")\n",
    "        \n",
    "        one_hot = np.zeros(self.action_size, dtype=np.uint8)\n",
    "        one_hot[self.move_to_index[move_uci]] = 1\n",
    "        return one_hot\n",
    "\n",
    "    def decode_move(self, index):\n",
    "        \"\"\"Décodage d'un index en coup UCI.\"\"\"\n",
    "        if index < 0 or index >= self.action_size:\n",
    "            raise ValueError(f\"Index de mouvement hors limite : {index}\")\n",
    "        \n",
    "        move_uci = self.index_to_move[index]\n",
    "        return chess.Move.from_uci(move_uci)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\n",
      "rnbqkbnr/1ppppppp/8/p7/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "board = ChessRL()\n",
    "\n",
    "state = board.get_initial_state()\n",
    "\n",
    "print(state)\n",
    "\n",
    "valid_moves = board.get_valid_moves(state)\n",
    "move = np.where(valid_moves == 1)[0][1]\n",
    "\n",
    "\n",
    "next_state = board.get_next_state(state, move, 1)\n",
    "\n",
    "change_perspective = board.change_perspective(next_state, -1)\n",
    "print(change_perspective)\n",
    "\n",
    "print(board.get_valid_moves(change_perspective).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapté à 22 channels, tout est encodé \n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, game, num_resBlocks, num_hidden, device):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(12, num_hidden, kernel_size = 3, padding=1),\n",
    "            nn.BatchNorm2d(num_hidden),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "\n",
    "        self.backBone = nn.ModuleList([ResBlock(num_hidden) for i in range(num_resBlocks)])\n",
    "\n",
    "        self.policyHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32*game.row_count * game.column_count, game.action_size)\n",
    "            )\n",
    "        \n",
    "        self.valueHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 3, kernel_size = 3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3*game.row_count*game.column_count, 1),\n",
    "            nn.Tanh()\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.startBlock(x)\n",
    "        for block in self.backBone:\n",
    "            x = block(x)\n",
    "        policy = self.policyHead(x)\n",
    "        value = self.valueHead(x)\n",
    "        return policy, value\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size = 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size = 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        return self.relu(out + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Node:\n",
    "    def __init__(self, game, args, state, parent= None, action_taken=None, prior=0, visit_count=0):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.prior = prior # probability of selecting this node (from the policy)\n",
    "\n",
    "        self.children = []\n",
    "\n",
    "        self.value_sum = 0\n",
    "        self.visit_count = visit_count\n",
    "    \n",
    "    def is_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def select(self):\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "\n",
    "        for child in self.children:\n",
    "            ucb = self.calculate_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_ucb = ucb\n",
    "                best_child = child\n",
    "        \n",
    "        return best_child\n",
    "\n",
    "    def calculate_ucb(self, child):\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2 \n",
    "        return q_value + self.args['C'] * np.sqrt((self.visit_count) / (child.visit_count + 1)) * child.prior\n",
    "\n",
    "    def expand(self, policy):\n",
    "\n",
    "        for action, prob in enumerate(policy):\n",
    "            if prob > 0:\n",
    "\n",
    "                board = chess.Board(self.state)\n",
    "\n",
    "                # print(board)\n",
    "\n",
    "                board_copy = board.copy()\n",
    "                child_state = board_copy.fen()\n",
    "\n",
    "                # action = np.zeros(self.game.action_size)\n",
    "                # action[action] = 1\n",
    "\n",
    "                # print(action)\n",
    "\n",
    "                child_state = self.game.get_next_state(child_state, action, 1)\n",
    "                child_state = self.game.change_perspective(child_state, player=-1)\n",
    "                child = Node(self.game, self.args, child_state, self, action, prob)\n",
    "                self.children.append(child)\n",
    "\n",
    "        return child\n",
    "    \n",
    "    \n",
    "    def backpropagation(self, value):\n",
    "        node = self\n",
    "        while node is not None:\n",
    "            node.visit_count += 1\n",
    "            node.value_sum += value\n",
    "\n",
    "            value = self.game.get_opponent_value(value)\n",
    "            node = node.parent\n",
    "\n",
    "class MCTSParallel:\n",
    "\n",
    "    def __init__(self, game, args,model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model.to(model.device)\n",
    "    \n",
    "    # Use for prediction, not training\n",
    "    @torch.no_grad()\n",
    "    def search(self, states, spGames):\n",
    "\n",
    "        liste = []\n",
    "\n",
    "        for state in states:\n",
    "            liste.append(self.game.get_encoded_state(state))\n",
    "        \n",
    "        liste = np.array(liste)\n",
    "        states_tensor = torch.tensor(\n",
    "            liste,\n",
    "            dtype=torch.float32\n",
    "        ).to(self.model.device)\n",
    "\n",
    "\n",
    "        # states_tensor = torch.tensor(\n",
    "        #     self.game.get_encoded_state(states),\n",
    "        #     dtype=torch.float32\n",
    "        # ).to(self.model.device)\n",
    "        \n",
    "        policy, _ = self.model(states_tensor)\n",
    "        policy = torch.softmax(policy, axis = 1).detach().cpu().numpy()\n",
    "\n",
    "        # add some noise to the policy to encourage exploration (dirichlet noise)\n",
    "        policy = (1 - self.args['epsilon']) * policy + self.args['epsilon'] * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size, size = policy.shape[0])\n",
    "\n",
    "\n",
    "        for i, spg in enumerate(spGames):\n",
    "            spg_policy = policy[i]\n",
    "\n",
    "            valid_moves = self.game.get_valid_moves(states[i])\n",
    "\n",
    "\n",
    "            # print(\"spg_policy\", spg_policy)\n",
    "            # print(\"valid_moves\", valid_moves.sum())\n",
    "\n",
    "            # renormalize\n",
    "            spg_policy = spg_policy / np.sum(spg_policy)\n",
    "\n",
    "\n",
    "            spg.root = Node(self.game, self.args, states[i], visit_count=1)\n",
    "\n",
    "            print(\"spg_policy\", spg_policy.sum())\n",
    "            spg.root.expand(spg_policy)\n",
    "\n",
    "\n",
    "        for search in range(self.args['num_searches']):\n",
    "            for i, spg in enumerate(spGames):\n",
    "                spg.node = None\n",
    "                node = spg.root\n",
    "\n",
    "                while node.is_expanded():\n",
    "\n",
    "                    node = node.select()\n",
    "                \n",
    "                value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "                value = self.game.get_opponent_value(value)\n",
    "\n",
    "                if is_terminal:\n",
    "                    node.backpropagation(value)\n",
    "                else: \n",
    "                    spg.node = node\n",
    "\n",
    "            expandable_spGames = [mappingIdx for mappingIdx in range(len(spGames)) if spGames[mappingIdx].node is not None]\n",
    "\n",
    "            if len(expandable_spGames) > 0:\n",
    "                states = np.stack([spGames[mappingIdx].node.state for mappingIdx in expandable_spGames])\n",
    "\n",
    "                # Apply get_encoded_state to all states (mapping=)\n",
    "                liste = []\n",
    "                for state in states:\n",
    "                    liste.append(self.game.get_encoded_state(state))\n",
    "                liste = np.array(liste)\n",
    "\n",
    "                states_tensor = torch.tensor(\n",
    "                    liste,\n",
    "                    dtype=torch.float32\n",
    "                ).to(self.model.device)\n",
    "\n",
    "                policy, value = self.model(\n",
    "                    states_tensor\n",
    "                )\n",
    "                \n",
    "                # policy, value = self.model(\n",
    "                #     torch.tensor(self.game.get_encoded_state(states), device=self.model.device)\n",
    "                # )\n",
    "\n",
    "                policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "                value = value.cpu().numpy()\n",
    "            \n",
    "            for i, mappingIdx in enumerate(expandable_spGames):\n",
    "                node = spGames[mappingIdx].node\n",
    "                spg_policy, spg_value = policy[i], value[i]\n",
    "                \n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                spg_policy *= valid_moves\n",
    "                spg_policy /= np.sum(spg_policy)\n",
    "\n",
    "                node.expand(spg_policy)\n",
    "                node.backpropagation(spg_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZeroParallel:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTSParallel(game, args, model)\n",
    "    \n",
    "    # Play multiple games at the same time\n",
    "    def selfPlay(self):\n",
    "        return_memory = []\n",
    "        player = 1\n",
    "        spGames = [SPG(self.game) for spg in range(self.args['num_processes'])]\n",
    "\n",
    "        while len(spGames) > 0:\n",
    "            states = np.stack([spg.state for spg in spGames])\n",
    "\n",
    "            liste = []\n",
    "            for state in states:\n",
    "                liste.append(self.game.change_perspective(state, player))\n",
    "            liste = np.array(liste)\n",
    "\n",
    "            neutral_states = liste\n",
    "            \n",
    "            # neutral_states = self.game.change_perspective(states, player)\n",
    "\n",
    "            self.mcts.search(neutral_states, spGames)\n",
    "\n",
    "\n",
    "            for i in range(len(spGames))[::-1]:\n",
    "\n",
    "                spg = spGames[i]\n",
    "\n",
    "                action_probs = np.zeros(self.game.action_size)\n",
    "                for child in spg.root.children:\n",
    "                    action_probs[child.action_taken] = child.visit_count\n",
    "                action_probs = action_probs / np.sum(action_probs)\n",
    "\n",
    "\n",
    "                spg.memory.append([spg.root.state, action_probs, player])\n",
    "\n",
    "                temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
    "                temperature_action_probs /= np.sum(temperature_action_probs)\n",
    "\n",
    "                action = np.random.choice(self.game.action_size, p=temperature_action_probs)\n",
    "\n",
    "                spg.state = self.game.get_next_state(spg.state, action, player)\n",
    "\n",
    "                value, is_terminal = self.game.get_value_and_terminated(spg.state, action)\n",
    "\n",
    "                if is_terminal:\n",
    "                    for hist_neutral_state, hist_action_probs, hist_player in spg.memory:\n",
    "\n",
    "                        # adapted, more general and work for 1 player game\n",
    "                        hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "                        \n",
    "                        return_memory.append([\n",
    "                            self.game.get_encoded_state(hist_neutral_state),\n",
    "                            hist_action_probs,\n",
    "                            hist_outcome\n",
    "                        ])\n",
    "                    del spGames[i]\n",
    "                \n",
    "            player = self.game.get_opponent(player)\n",
    "        \n",
    "        return return_memory\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, memory):\n",
    "        \n",
    "        random.shuffle(memory)\n",
    "\n",
    "        for batchidx in range(0, len(memory), self.args['batch_size']):\n",
    "            batch = memory[batchidx:min(len(memory) + 1,batchidx+self.args['batch_size'])]\n",
    "\n",
    "            # list of list, convert to tensor, these are the targets for policy and value\n",
    "            states, action_probs, outcomes = zip(*batch)\n",
    "\n",
    "            states, action_probs, outcomes = np.array(states), np.array(action_probs), np.array(outcomes).reshape(-1, 1)\n",
    "\n",
    "            states = torch.tensor(states, dtype=torch.float32).to(self.model.device)\n",
    "            action_probs = torch.tensor(action_probs, dtype=torch.float32).to(self.model.device)\n",
    "            outcomes = torch.tensor(outcomes, dtype=torch.float32).to(self.model.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            policy, value = self.model(states)\n",
    "\n",
    "            value_loss = F.mse_loss(value, outcomes)\n",
    "            policy_loss = F.cross_entropy(policy, action_probs)\n",
    "\n",
    "            loss = value_loss + policy_loss\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            memory = []\n",
    "            \n",
    "            self.model.eval()\n",
    "\n",
    "            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations'] // self.args['num_processes']):\n",
    "                memory += self.selfPlay()\n",
    "\n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "\n",
    "            torch.save(self.model.state_dict(), f'save\\model_{iteration}.pth')\n",
    "            torch.save(self.optimizer.state_dict(), f'save\\optimizer_{iteration}.pth')\n",
    "\n",
    "# self play game\n",
    "class SPG:\n",
    "    def __init__(self, game):\n",
    "        self.state = game.get_initial_state()\n",
    "        self.memory = []\n",
    "        self.root = None\n",
    "        self.node = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b966fc47ca84434fa73b506f32914a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spg_policy 1.0\n"
     ]
    },
    {
     "ename": "InvalidMoveError",
     "evalue": "invalid uci (use 0000 for null moves): 'a2a2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidMoveError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 24\u001b[0m\n\u001b[0;32m     10\u001b[0m args \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_searches\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m600\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirichlet_alpha\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.3\u001b[39m\n\u001b[0;32m     21\u001b[0m }\n\u001b[0;32m     23\u001b[0m alphaZero \u001b[38;5;241m=\u001b[39m AlphaZeroParallel(model, optimizer, game, args)\n\u001b[1;32m---> 24\u001b[0m \u001b[43malphaZero\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[70], line 107\u001b[0m, in \u001b[0;36mAlphaZeroParallel.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m selfPlay_iteration \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_selfPlay_iterations\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_processes\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m--> 107\u001b[0m     memory \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselfPlay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n",
      "Cell \u001b[1;32mIn[70], line 27\u001b[0m, in \u001b[0;36mAlphaZeroParallel.selfPlay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     23\u001b[0m neutral_states \u001b[38;5;241m=\u001b[39m liste\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# neutral_states = self.game.change_perspective(states, player)\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmcts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneutral_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspGames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(spGames))[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m     32\u001b[0m     spg \u001b[38;5;241m=\u001b[39m spGames[i]\n",
      "File \u001b[1;32mc:\\Users\\gprad\\anaconda3\\envs\\alpha-zero\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[72], line 122\u001b[0m, in \u001b[0;36mMCTSParallel.search\u001b[1;34m(self, states, spGames)\u001b[0m\n\u001b[0;32m    119\u001b[0m     spg\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m Node(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, states[i], visit_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspg_policy\u001b[39m\u001b[38;5;124m\"\u001b[39m, spg_policy\u001b[38;5;241m.\u001b[39msum())\n\u001b[1;32m--> 122\u001b[0m     \u001b[43mspg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspg_policy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m search \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_searches\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, spg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(spGames):\n",
      "Cell \u001b[1;32mIn[72], line 54\u001b[0m, in \u001b[0;36mNode.expand\u001b[1;34m(self, policy)\u001b[0m\n\u001b[0;32m     47\u001b[0m child_state \u001b[38;5;241m=\u001b[39m board_copy\u001b[38;5;241m.\u001b[39mfen()\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# action = np.zeros(self.game.action_size)\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# action[action] = 1\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# print(action)\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m child_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_next_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m child_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mchange_perspective(child_state, player\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     56\u001b[0m child \u001b[38;5;241m=\u001b[39m Node(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, child_state, \u001b[38;5;28mself\u001b[39m, action, prob)\n",
      "Cell \u001b[1;32mIn[45], line 57\u001b[0m, in \u001b[0;36mChessRL.get_next_state\u001b[1;34m(self, state, action, player)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_next_state\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, action, player):\n\u001b[0;32m     56\u001b[0m     board \u001b[38;5;241m=\u001b[39m chess\u001b[38;5;241m.\u001b[39mBoard(state)\n\u001b[1;32m---> 57\u001b[0m     move \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_move\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m move \u001b[38;5;129;01min\u001b[39;00m board\u001b[38;5;241m.\u001b[39mlegal_moves:\n\u001b[0;32m     59\u001b[0m         board\u001b[38;5;241m.\u001b[39mpush(move)\n",
      "Cell \u001b[1;32mIn[45], line 124\u001b[0m, in \u001b[0;36mChessRL.decode_move\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex de mouvement hors limite : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    123\u001b[0m move_uci \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_to_move[index]\n\u001b[1;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mchess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMove\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_uci\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmove_uci\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gprad\\anaconda3\\envs\\alpha-zero\\lib\\site-packages\\chess\\__init__.py:709\u001b[0m, in \u001b[0;36mMove.from_uci\u001b[1;34m(cls, uci)\u001b[0m\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidMoveError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid uci: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muci\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    708\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m from_square \u001b[38;5;241m==\u001b[39m to_square:\n\u001b[1;32m--> 709\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidMoveError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid uci (use 0000 for null moves): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muci\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(from_square, to_square, promotion\u001b[38;5;241m=\u001b[39mpromotion)\n\u001b[0;32m    711\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mInvalidMoveError\u001b[0m: invalid uci (use 0000 for null moves): 'a2a2'"
     ]
    }
   ],
   "source": [
    "game = ChessRL()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet(game, 4, 64, device)\n",
    "\n",
    "# temperature ->  eploitation / exploration tradeoff, same role as gamma in Q-learning. High temperature -> more exploration (rd distribution), low temperature -> more exploitation (peak distribution)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    " \n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 600,\n",
    "    'num_iterations': 8,\n",
    "    'num_selfPlay_iterations': 10,\n",
    "    'num_processes': 10,\n",
    "    'num_epochs': 4,\n",
    "    'batch_size': 128,\n",
    "    'temperature': 1.25,\n",
    "    'epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "alphaZero = AlphaZeroParallel(model, optimizer, game, args)\n",
    "alphaZero.learn()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpha-zero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
